# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the HoneyBee License found in the
# LICENSE file in the root directory of this source tree.

name: <run_name>
dump_dir: <savepath>
steps: 6075 # 5 (epochs) * 1215 (steps); 1215 = (2.5M / 2048) ; 2048 = 16 (nodes) * 8 (GPUs) * 2 (bs) * 8 (grad-acc)
seed: 777
grad_acc_steps: 8
optim:
  lr: 1.0e-05
  warmup: 607
  lr_min_ratio: 0.01
  clip: 1.0
  weight_decay: 0.05
distributed:
  fsdp_type: full_shard
  compile: false
  model_dtype: bf16
  matmul_allow_tf32: false
  selective_activation_checkpointing: false
  full_activation_checkpointing: true
  tp_size: 1
model:
  dim: 4096
  n_layers: 32
  n_heads: 32
  n_kv_heads: 8
  vocab_size: 128256
  ffn_dim_multiplier: 1.3
  multiple_of: 1024
  norm_eps: 1.0e-05
  rope_theta: 500000.0
  weight_tying: false
  max_seqlen: 11520
  freeze_language_model: false
  freeze_vision_model: false
  pooling_ratio: 2
  vision_model:
    image_size: 448
    patch_size: 14
    width: 1536
    layers: 47
    heads: 16
    use_cls_token: false
    use_abs_posemb: true
    mlp_ratio: 5.833333334
    ls_init_value: 0.1
    drop_path: 0.1
    use_ln_post: false
    pool_type: none
  mlp_init:
    use_gaussian: true
data:
  datamix: 'honeybee: 1'
  num_workers: 4
  batch_size: 2
  image_res: 448
  max_num_tiles: 36
  max_video_frames: 32
  vision_input_type: thumb+tile
  tokenizer_path: <path to Perception-LM-8B/tokenizer.model>
  tokenizer_name: plmchat
  conversation_format: plm_sft
profiling:
  run: false
checkpoint:
  dump:
    every: 303
  init_ckpt_path: <path to Perception-LM-8B>
  is_consolidated_model: true
logging:
  freq: 10
  level: INFO
