# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the HoneyBee License found in the
# LICENSE file in the root directory of this source tree.

name: <run_name>
dump_dir: <savepath>
steps: 6075 # 5 (epochs) * 1215 (steps); 1215 = (2.5M / 2048) ; 2048 = 8 (nodes) * 8 (GPUs) * 4 (bs) * 8 (grad-acc)
seed: 777
grad_acc_steps: 8
optim:
  lr: 4.0e-05
  warmup: 607 # 10% warmup
  lr_min_ratio: 0.01
  clip: 1.0
  weight_decay: 0.01
distributed:
  fsdp_type: full_shard
  compile: false
  model_dtype: bf16
  matmul_allow_tf32: false
  selective_activation_checkpointing: false
  full_activation_checkpointing: true
  tp_size: 1
model:
  dim: 2048
  n_layers: 16
  n_heads: 32
  n_kv_heads: 8
  vocab_size: 128256
  ffn_dim_multiplier: 1.5
  multiple_of: 256
  norm_eps: 1.0e-05
  rope_theta: 500000.0
  weight_tying: true
  rope_scale_factor: 32
  high_freq_factor: 4
  max_seqlen: 11520
  freeze_language_model: false
  freeze_vision_model: false
  pooling_ratio: 2
  vision_model:
    image_size: 448
    patch_size: 14
    width: 1024
    layers: 23
    heads: 16
    use_cls_token: true
    use_abs_posemb: true
    mlp_ratio: 4.0
    ls_init_value: 0.1
    drop_path: 0.1
    use_ln_post: false
    pool_type: none
  mlp_init:
    use_gaussian: true
data:
  datamix: 'honeybee: 1' # add honeybee in datasets.yaml
  num_workers: 4
  batch_size: 4
  image_res: 448
  max_num_tiles: 36
  max_video_frames: 32
  vision_input_type: thumb+tile
  tokenizer_path: <path to Perception-LM-1B/tokenizer.model>
  tokenizer_name: plmchat
  conversation_format: plm_sft
profiling:
  run: false
checkpoint:
  dump:
    every: 303 ## save frequency
  init_ckpt_path: <path to Perception-LM-1B>
  is_consolidated_model: true
logging:
  freq: 10
  level: INFO
